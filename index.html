<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>6.7960 Final Project Fall 2024</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Patch-Based Clustering Techniques for Facial Emotion Recognition</h1>
        <p>By Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan</p>
    </header>

    <nav>
        <a href="#introduction">Introduction</a>
        <a href="#methods">Methods</a>
        <a href="#results">Results</a>
        <a href="#conclusion">Conclusion</a>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>This project investigates how different image patch clustering algorithms affect the accuracy of facial emotion recognition. </p>
        </section>

        <section id="methods">
            <h2>Methods</h2>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 1: Architecture Flow</h3>
                    <img src="images/DLDiagram.png" alt="Model Arch">
                </div>
            </div>
            <p>Our preliminary analysis of the generated embeddings reveals the following insights:</p>
            <ul>
                <li><b>Clustering Patterns:</b> Both PCA and t-SNE visualizations show distinct regions, suggesting potential grouping of embeddings based on musical characteristics or metadata (e.g., genre or human vs. AI-generated labels).
</li>
                <li><b>Separation in t-SNE:</b> The t-SNE plot highlights better local relationships, indicating that embeddings capture nuanced features effectively, making it suitable for identifying similar audio samples.
</li>
                <li><b>Outlier Detection:</b> Sparse points in the plots may correspond to outliers or unique embeddings, potentially representing uncommon musical patterns or anomalies in the dataset.
</li>
                <li><b>Next Steps:</b> Incorporating metadata (e.g., genres, repetition scores) as color-coded labels could reveal additional patterns and validate how well embeddings align with these attributes.
</li>
            </ul>
        </section>

        <section id="results">
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 3</h3>
                    <img src="images/placeholder-image.jpg" alt="Media Lab Study Audio">
                </div>
                <div class="figure-item">
                    <h3>Figure 4:</h3>
                    <img src="images/placeholder-image.jpg" alt="Dataset Audio">
                </div>
            </div>
            <p>Our preliminary analysis reveals the following insights:</p>
            <ul>
                <li>AI-generated music tends to have more repetitive patterns.</li>
                <li>Human-composed music demonstrates greater adherence to themes and more emotion.</li>
                <li>Listeners rated AI-generated music as "very generic."</li>
            </ul>
            <p>Our findings show that there is a struggle to distinguish between the pieces and we will further investigate the results as well as potentially run another larger scale study to learn more.</p>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>Our findings highlight the potential for AI-generated music to evolve with more nuanced training. Future work will refine these models and incorporate deeper listener feedback.</p>
        </section>
    </main>

    <footer>
        <p>&copy; Fall 2024 Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan | Contact: <a href="mailto:ananya_k@mit.edu">ananya_k@mit.edu</a>, <a href="mailto:brishika@mit.edu">klecamwa@mit.edu</a>, <a href="mailto:klecamwa@mit.edu">sharvaa@mit.edu</a></p>
    </footer>
</body>
</html>

