<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>6.7960 Final Project Fall 2024</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>Patch-Based Clustering Techniques for Facial Emotion Recognition</h1>
        <p>By Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan</p>
    </header>

    <nav>
        <a href="#introduction">Introduction</a>
        <a href="#preprocessing">Preprocessing Data</a>
        <a href="#methods">Methods</a>
        <a href="#results">Results</a>
        <a href="#conclusion">Conclusion</a>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>This project investigates how different image patch clustering algorithms affect the accuracy of facial emotion recognition. </p>
        </section>

        <section id="preprocessing">
            
            <h2>Dataset and Preprocessing</h2>
            <section>
                <h3 class="subheading">Dataset Description</h3>
                <p>
                    The dataset used for this project is derived from the 
                    <a href="https://www.kaggle.com/datasets/noamsegal/affectnet-training-data" target="_blank">AffectNet Training Data</a>, 
                    one of the largest datasets for facial emotion recognition. It provides labeled examples of human faces for eight primary emotions: 
                    <strong>Anger, Contempt, Disgust, Fear, Happiness, Neutral, Sadness,</strong> and <strong>Surprise</strong>.
                </p>
                <p>
                    AffectNet stands out due to its diversity, with faces captured in real-world environments under varying lighting, poses, and demographics. 
                    This makes it an ideal dataset for training models capable of generalizing across diverse conditions.
                </p>
            </section>
    
            <section>
                <h3 class="subheading">Challenges in the Dataset</h3>
                <p>
                    Despite its richness, AffectNet presents significant challenges:
                </p>
                <ul>
                    <li><strong>Class Imbalance:</strong> Certain emotions like "Happiness" dominate the dataset, while others like "Disgust" are underrepresented.</li>
                    <li><strong>Variability:</strong> Differences in lighting, facial occlusions, and pose angles add noise to the dataset.</li>
                    <li><strong>Subjectivity:</strong> Emotions are inherently subjective, and labeling inconsistencies may arise across annotators.</li>
                </ul>
            </section>
    
            <section>
                <h3 class="subheading">Original Emotion Distribution</h3>
                <p>
                    The original dataset exhibits a skewed distribution across emotion classes, as visualized in the chart below:
                </p>
                <div class="bar-chart">
                    <img src="images/Data_Distribution.png" alt="Bar chart showing original emotion distribution">
                </div>
            </section>
    
            <section>
                <h3 class="subheading">Balanced Dataset</h3>
                <p>
                    To address the class imbalance, we balanced the dataset by subsampling 
                    <strong>2,477 images</strong> per class, resulting in a total of <strong>19,816 images</strong>. 
                    This ensures fair representation across all emotions and prevents the model from being biased toward overrepresented classes.
                </p>
            </section>
            <section>
                <h3 class="subheading">Example Images</h3>
                <p>
                    Below are sample images from the dataset, showcasing one example from each emotion class, labeled for clarity:
                </p>
                <div class="grid-container">
                    <div class="grid-item">
                        <img src="images/anger.jpg" alt="Anger Example">
                        <p class="label">Anger</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/contempt.png" alt="Contempt Example">
                        <p class="label">Contempt</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/disgust.jpg" alt="Disgust Example">
                        <p class="label">Disgust</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/fear.jpg" alt="Fear Example">
                        <p class="label">Fear</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/happy.png" alt="Happiness Example">
                        <p class="label">Happiness</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/neutral.png" alt="Neutral Example">
                        <p class="label">Neutral</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/sad.jpg" alt="Sad Example">
                        <p class="label">Sad</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/surprise.png" alt="Surprise Example">
                        <p class="label">Surprise</p>
                    </div>
                </div>
            </section>
            </section>
                <h3 class="subheading">CNN Embeddings</h3>
                <p> 
                    We first ensured all images were indeed of size 96 by 96. After that we split teh image into 36 patches of dimensions 16 by 16 each. Each image was now represented by 36 image patches.
                </p>
                <p>
                    As part of our methodolgy, we utilize a pretrained <strong>ResNet-50</strong> model to generate 
                    2048-dimensional feature embeddings for each image patch. These embeddings are extracted from the 
                    global average pooling layer of the network, which ensures compact and informative representations 
                    of visual features.
                </p>
                <p>
                    These embeddings serve as the foundation for our clustering techniques. By leveraging transfer 
                    learning, we efficiently extract high-level features, reducing computational overhead while 
                    maintaining rich feature representations for the downstream clustering tasks.
                </p>
            </section>

        <section id="methods">
            <h2>Methods</h2>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 1: Architecture Flow</h3>
                    <img src="images/DL_Arch_Final.png" alt="Model Arch">
                </div>
            </div>
    <section>
        <h3 class="subheading">Patch-to-Cluster Attention</h3>
        <p>
            The patch-to-cluster attention mechanism plays a crucial role in enhancing the interpretability of the model by 
            linking individual image patches to specific clusters. The attention mechanism operates as follows:
        </p>
        <ul>
            <li><strong>Query, Key, and Value Projections:</strong> Patch embeddings are projected into query vectors (Q) using a linear layer. Cluster centroids, computed via a clustering algorithm, are projected into key (K) and value (V) vectors. These projections are used to calculate the relationships between patches and clusters.</li>
            <li><strong>Attention Computation:</strong> Attention scores are computed using a scaled dot-product where N is the number of patches, C is the embedding dimensionality, and M is the number of clusters.
                \[
                    A_{N,M} = \text{Softmax}\left( \frac{Q_{N,C} \cdot K_{M,C}^T}{\sqrt{C}} \right)_{\text{dim}=1}
                \]
            </li>
            <li><strong>Weighted Aggregation:</strong> The attention scores weigh the cluster value vectors (V). This step integrates global cluster-level information into the patch representations.</li>
            <li><strong>Output:</strong> The aggregated patch representations are transformed via a linear output projection to produce the final enhanced patch embeddings.</li>
        </ul>
    </section>
    <section>
        <h3 class="subheading">Vision Transformer with Patch-to-Cluster Attention</h3>
        <p>
            This mechanism enables the model to focus on relevant clusters, improving sensitivity to localized features in facial expressions.
        </p>
    </section>
    <section>
        <h3 class="subheading">Patch Clustering</h3>
        <ul>
            <li>
                <strong>K-Means Clustering:</strong> 
                Method: K-means partitions data into clusters by iteratively minimizing the variance within clusters and updating centroids. 
                Advantages: Simple, fast, and effective for well-separated, spherical clusters.
            </li>
            <li>
                <strong>Spectral Clustering:</strong>
                Method: Spectral clustering constructs a similarity graph, performs eigenvector decomposition, and clusters data in a low-dimensional space. 
                Advantages: Works well for non-linear and arbitrarily shaped clusters.
            </li>
            <li>
                <strong>Agglomerative Clustering:</strong>
                Method: Agglomerative clustering is a hierarchical approach that recursively merges the most similar data points or clusters until a stopping criterion is met. 
                Advantages: Does not require pre-specifying the number of clusters and captures hierarchical relationships.
            </li>
        </ul>
    </section>
    <section>
        <h3 class="subheading">Experimental Setup</h3>
        <ul>
            <li>
                <strong>Cluster Sizes:</strong> For each clustering method (K-means, spectral clustering, and agglomerative clustering), we test three cluster sizes: 2, 4, and 8. 
                The choice of cluster size balances interpretability and computational efficiency.
            </li>
            <li>
                <strong>Training and Testing:</strong> The generated embeddings are clustered using each method, and cluster centroids are fed into the patch-to-cluster attention module. 
                The model is trained to classify the facial images into one of eight emotion categories, using AffectNet's labeled data for supervised learning. A validation set is used to monitor overfitting and tune hyperparameters.
            </li>
            <li>
                <strong>Evaluation Metrics:</strong> Classification accuracy on the AffectNet test set.
            </li>
        </ul>
        <p>
            By systematically varying cluster sizes and clustering methods, we aim to identify configurations that maximize recognition accuracy 
            while providing interpretable insights into the role of clusters in emotion recognition.
        </p>

        <section id="results">
            <h1>Clustering Comparison Table</h1>
            <table class="results-table">
                <thead>
                    <tr>
                        <th>Clusters</th>
                        <th>KMeans</th>
                        <th>Agglomerative</th>
                        <th>Spectral</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>2</td>
                        <td><b>33.65%</b></td>
                        <td>28.71%</td>
                        <td><b>44.00%</b></td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>26.46%</td>
                        <td><b>41.42%</b></td>
                        <td>38.75%</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>31.76%</td>
                        <td>25.38%</td>
                        <td>33.00%</td>
                    </tr>
                </tbody>
            </table>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 3</h3>
                    <img src="images/placeholder-image.jpg" alt="Media Lab Study Audio">
                </div>
                <div class="figure-item">
                    <h3>Figure 4:</h3>
                    <img src="images/placeholder-image.jpg" alt="Dataset Audio">
                </div>
            </div>
            <p>Our preliminary analysis reveals the following insights:</p>
            <ul>
                <li>AI-generated music tends to have more repetitive patterns.</li>
                <li>Human-composed music demonstrates greater adherence to themes and more emotion.</li>
                <li>Listeners rated AI-generated music as "very generic."</li>
            </ul>
            <p>Our findings show that there is a struggle to distinguish between the pieces and we will further investigate the results as well as potentially run another larger scale study to learn more.</p>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>Our findings highlight the potential for AI-generated music to evolve with more nuanced training. Future work will refine these models and incorporate deeper listener feedback.</p>
        </section>
    </main>

    <footer>
        <p>&copy; Fall 2024 Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan | Contact: <a href="mailto:ananya_k@mit.edu">ananya_k@mit.edu</a>, <a href="mailto:brishika@mit.edu">brishika@mit.edu</a>, <a href="mailto:sharvaa@mit.edu">sharvaa@mit.edu</a></p>
    </footer>
</body>
</html>

