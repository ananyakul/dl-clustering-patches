<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>6.7960 Final Project Fall 2024</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>Patch-Based Clustering Techniques for Facial Emotion Recognition</h1>
        <p>By Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan</p>
    </header>

    <nav>
        <a href="#introduction">Introduction</a>
        <a href="#preprocessing">Data</a>
        <a href="#methods">Methods</a>
        <a href="#experiments">Experiments</a>
        <a href="#conclusion">Conclusion</a>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <section>
                <h3 class="subheading">Background</h3>
                <p>Emotion recognition has emerged as a pivotal task in the field of computer vision, with applications ranging from social robots to mental health diagnostics [1]. By analyzing facial expressions, emotion recognition systems enable machines to understand human emotions in real-time, fostering more natural and empathetic interactions. These systems are also transforming industries such as healthcare, where robots are increasingly able to diagnose and monitor mental health conditions, alerting human doctors of emergencies before they occur [2]. </p>
                <p>One of the most innovative approaches in computer vision is the use of Vision Transformers (ViTs), which are based on the Transformer models that revolutionized natural language processing. In recent years, ViTs have been leveraged for computer vision tasks including high-level vision, image recognition, feature extraction, and video processing, performing similarly to or better than traditional convolutional and recurrent neural networks (CNN/RNN) [3]. Complementary to ViTs, computer vision tasks increasingly rely on patching, a technique where images are divided into smaller, fixed-size pieces (“patches”) in order to focus on individual regions, striking a balance between analyzing individual pixels (too localized) and the entirety of the image (too broad). Therefore, patching can provide more fine-grained analysis of an image while capturing local relationships [4].</p>
                <p>Recent work has also experimented with clustering patches together in order to improve efficiency and group morphologically similar patches, thereby increasing the model’s understanding of the image [6]. Clustering algorithms for patches mimic the already well-known and common clustering methods used in machine learning, including: k-means clustering, spectral clustering, gaussian mixture models (GMMs), agglomerative hierarchical clustering, DBSCAN, and mean shift clustering [7].</p>
            </section>

            <section>
                <h3 class="subheading">Objective</h3>
                <p>Currently, ViTs utilize image patches as tokens for patch-to-patch attention. However, past work has shown a moderate increase in accuracy and decrease in complexity by clustering these patches together and using patch-to-cluster attention instead [5, 6]. This project investigates the impact of various patch clustering techniques on emotion recognition accuracy, using the AffectNet dataset as our primary benchmark. Our goal is to identify which clustering methodologies, and number of clusters, most effectively enhance emotion detection accuracy. As a secondary evaluation metric, we will be analyzing the trade-offs between recognition accuracy and time efficiency to understand which method is the most practical for real-world usage.</p>
                <p>We hypothesize that different clustering approaches will yield varying levels of accuracy in identifying emotions, with non-spherical clusters likely outperforming others. We also anticipate that the optimal number of clusters will strike a balance, resulting in high recognition accuracy by not creating too many or too few groups of facial features (patches).</p>
            </section>

            <section>
                <h3 class="subheading">Approach</h3>
                <p>The model pipeline we use includes two main parts: a clustering algorithm to cluster image patches, and a Vision Transformer (ViT) utilizing patch-to-cluster attention for emotion recognition accuracy results. The patch-to-cluster attention mechanism and the ViT hyperparameters remain fixed throughout this project, while the clustering algorithm and number of clusters are changed in order to isolate the effects of different patch clusterings on accuracy. The novelty of our approach lies in integrating patch-to-cluster attention with multiple clustering techniques to determine which method is most effective for emotion recognition. Although patch-to-cluster has been implemented before with CNN clustering [5], we expand upon this work by comparing multiple clustering algorithms (K-Means, Spectral, and Agglomerative).</p>
            </section>
        </section>

        <section id="preprocessing">
            
            <h2>Dataset and Preprocessing</h2>
            <section>
                <h3 class="subheading">Dataset Description</h3>
                <p>
                    The dataset used for this project is derived from the 
                    <a href="https://www.kaggle.com/datasets/noamsegal/affectnet-training-data" target="_blank">AffectNet Training Data</a>, 
                    one of the largest datasets for facial emotion recognition. It provides labeled examples of human faces for eight primary emotions: 
                    <strong>Anger, Contempt, Disgust, Fear, Happiness, Neutral, Sadness,</strong> and <strong>Surprise</strong>.
                </p>
                <p>
                    AffectNet stands out due to its diversity, with faces captured in real-world environments under varying lighting, poses, and demographics. 
                    This makes it an ideal dataset for training models capable of generalizing across diverse conditions.
                </p>
            </section>
    
            <section>
                <h3 class="subheading">Challenges in the Dataset</h3>
                <p>
                    Despite its richness, AffectNet presents significant challenges:
                </p>
                <ul>
                    <li><strong>Class Imbalance:</strong> Certain emotions like "Happiness" dominate the dataset, while others like "Disgust" are underrepresented.</li>
                    <li><strong>Variability:</strong> Differences in lighting, facial occlusions, and pose angles add noise to the dataset.</li>
                    <li><strong>Subjectivity:</strong> Emotions are inherently subjective, and labeling inconsistencies may arise across annotators.</li>
                </ul>
            </section>
    
            <section>
                <h3 class="subheading">Original Emotion Distribution</h3>
                <p>
                    The original dataset exhibits a skewed distribution across emotion classes, as visualized in the chart below:
                </p>
                <div class="figure-container">
                    <div class="figure-item">
                        <img src="images/Data_Distribution.png" alt="Bar chart showing original emotion distribution">
                        <div class="figure-caption">Figure 1: Original Data Distribution Across Classes</div>
                    </div>
                </div>
            </section>
    
            <section>
                <h3 class="subheading">Balanced Dataset</h3>
                <p>
                    To address the class imbalance, we balanced the dataset by subsampling 
                    <strong>2,477 images</strong> per class, resulting in a total of <strong>19,816 images</strong>. 
                    This ensures fair representation across all emotions and prevents the model from being biased toward overrepresented classes.
                </p>
            </section>
            <section>
                <h3 class="subheading">Example Images</h3>
                <p>
                    Below are sample images from the dataset, showcasing one example from each emotion class, labeled for clarity:
                </p>
                <div class="example-grid">
                    <div class="grid-item">
                        <img src="images/anger.jpg" alt="Anger Example">
                        <p class="label">Anger</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/contempt.png" alt="Contempt Example">
                        <p class="label">Contempt</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/disgust.jpg" alt="Disgust Example">
                        <p class="label">Disgust</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/fear.jpg" alt="Fear Example">
                        <p class="label">Fear</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/happy.png" alt="Happiness Example">
                        <p class="label">Happiness</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/neutral.png" alt="Neutral Example">
                        <p class="label">Neutral</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/sad.jpg" alt="Sad Example">
                        <p class="label">Sad</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/surprise.png" alt="Surprise Example">
                        <p class="label">Surprise</p>
                    </div>
                </div>
            </section>
            </section>
                <h3 class="subheading">CNN Embeddings</h3>
                <p> 
                    We first ensured all images were indeed of size 96 by 96. After that we split the image into 36 patches of dimensions 16 by 16 each. Each image was now represented by 36 image patches.
                </p>
                <p>
                    As part of our methodolgy, we utilize a pretrained <strong>ResNet-50</strong> model to generate 
                    2048-dimensional feature embeddings for each image patch. These embeddings are extracted from the 
                    global average pooling layer of the network, which ensures compact and informative representations 
                    of visual features. These embeddings serve as the foundation for our clustering techniques.
                </p>
            </section>

        <section id="methods">
            <h2>Methods</h2>
            <div class="figure-container large-image">
                <div class="figure-item">
                    <img src="images/DL_Arch_Final.png" alt="Model Arch">
                    <div class="figure-caption">Figure 2: Architecture Flow</div>
                </div>
            </div>
    <section>
        <h3 class="subheading">Vision Transformer Architecture</h3>
        <p>
            Vision Transformers (ViTs) adapt the traditional transformer model to make them applicable to computer vision tasks. Unlike convolutional neural networks, which rely on local receptive fields, ViTs divide images into fixed-size patches and treat them as a sequence of tokens which are linearly projected into an embedding space. The self-attention mechanism in ViTs then computes relationships between all pairs of patches (patch-to-patch attention). A learnable class token aggregates information from all patches, and its output is used for classification tasks. ViTs excel at capturing long-range dependencies and global patterns in image data, making them particularly effective for tasks requiring detailed contextual understanding. 
        </p>
        <p>
            A well known issue with patch-to-patch attention is that it suffers from quadratic complexity [5], since every query must interact with every key. Our architecture instead uses Patch-to-Cluster attention, where a predefined number of cluster assignments is first learned and then used in computing the key and value, resulting in linear complexity.
        </p>
        <div class="grid-container grid-2-columns">
            <div class="grid-item">
                <img src="images/Patch-to-Patch.jpg" alt="Patch-to-Patch">
                <p class="label">Patch-to-Patch Attention [5] <br> (Traditional)</p>
            </div>
            <div class="grid-item">
                <img src="images/Patch-to-Cluster.jpg" alt="Patch-to-Cluster">
                <p class="label">Patch-to-Cluster Attention [5]  <br> (What We Use)</p>
            </div>
        </div>
    </section>
    <section>
        <h3 class="subheading">Patch-to-Cluster Attention</h3>
        <p>
            Our attention mechanism operates as follows:
        </p>
        <ul>
            <li><strong>Query, Key, and Value Projections:</strong> Patch embeddings are projected into query vectors (Q) using a linear layer. Cluster centroids, computed via a clustering algorithm, are projected into key (K) and value (V) vectors. These projections are used to calculate the relationships between patches and clusters.</li>
            <li><strong>Attention Computation:</strong> Attention scores are computed using a scaled dot-product where N is the number of patches, C is the embedding dimensionality, and M is the number of clusters.
                \[
                    A_{N,M} = \text{Softmax}\left( \frac{Q_{N,C} \cdot K_{M,C}^T}{\sqrt{C}} \right)_{\text{dim}=1}
                \]
            </li>
            <li><strong>Weighted Aggregation:</strong> The attention scores weigh the cluster value vectors (V). This step integrates global cluster-level information into the patch representations.</li>
            <li><strong>Output:</strong> The aggregated patch representations are transformed via a linear output projection to produce the final enhanced patch embeddings.</li>
        </ul>
        <p>Our model architecture diverges from the traditional Vision Transformer design by employing patch-to-cluster attention (See Figure 2).</p>
    </section>
    
    <section>
        <h3 class="subheading">Patch Clustering</h3>
        We use the following clustering methods for the image patches [7]:
        <ul>
            <li>
                <strong>K-Means Clustering:</strong> iteratively partitions data into a predefined number of clusters by minimizing the variance within each cluster. Centroids are initialized randomly, and clusters are updated based on the distance of each data point to the nearest centroid. K-means is simple and computationally efficient, making it suitable for high-dimensional data. It is most effective for well-separated and spherical clusters.
            </li>
            <li>
                <strong>Agglomerative Clustering:</strong> starts with each data point as its own cluster and recursively merges the most similar clusters until a predefined number is reached. Notably, it is able to capture hierarchical relationships in the data and is also computationally efficient.
            </li>
            <li>
                <strong>Spectral Clustering:</strong> Spectral clustering constructs a similarity graph based on pairwise relationships between data points and performs eigenvector decomposition to embed the graph into a lower-dimensional space for clustering. It is known for handling complex, arbitrarily shaped clusters effectively. It is also suitable for datasets where clusters are not linearly separable.
            </li>
        </ul>
        <div class="grid-container grid-3-columns">
            <div class="grid-item">
                <img src="images/KMeans.png" alt="KMeans Clustering">
                <p class="label">KMeans Clustering</p>
            </div>
            <div class="grid-item">
                <img src="images/Agglo.png" alt="Agglomerative Clustering">
                <p class="label">Agglomerative Clustering</p>
            </div>
            <div class="grid-item">
                <img src="images/Spectral.png" alt="Spectral Clustering">
                <p class="label">Spectral Clustering</p>
            </div>
        </div>
    </section>

            <section id="model-training">
                <h2>Model Training</h2>
                <ul>
                    <li>
                        <strong>Dataset and Preprocessing:</strong> The dataset is split into training and testing sets, with <strong>80%</strong> of the data used for training and <strong>20%</strong> for testing.
                    </li>
                    <li>
                        <strong>Training Setup:</strong> We train our model with the Adam optimizer using a learning rate of <strong>1e-3</strong>, <strong>CrossEntropyLoss</strong>, a depth of <strong>6</strong>, and <strong>30 epochs</strong> of training.
                    </li>
                    <li>
                        <strong>Device:</strong> Model training is done on <strong>Google Colab A100 GPU</strong>.
                    </li>
                </ul>
            </section>

        
        <section id="experiments">
            <h2>Experiments and Results</h2>
            <section>
                <h3 class="subheading">Experimental Setup</h3>
                <p>
                   We analyze the classification accuracy of our model across three clustering methods (K-means, spectral, and agglomerative) and three cluster sizes (2, 4, and 8). We evaluate the classification accuracies of our models on the AffectNet test set. We also evaluate the time complexity of each clustering method to better understand the tradeoff between accuracy and computational costs.
                </p>
            </section>

        <section>
            <h3 class="subheading">Results</h3>
            <table class="results-table">
                <thead>
                    <tr>
                        <th>Clusters</th>
                        <th>KMeans</th>
                        <th>Agglomerative</th>
                        <th>Spectral</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>2</td>
                        <td><b>33.65%</b></td>
                        <td>28.71%</td>
                        <td><b>44.00%</b></td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>26.46%</td>
                        <td><b>41.42%</b></td>
                        <td>38.75%</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>31.76%</td>
                        <td>25.38%</td>
                        <td>33.00%</td>
                    </tr>
                </tbody>
            </table>
            <p>
               Our results reveal significant differences in performance among the clustering methods across cluster sizes (k). At k=2, Spectral Clustering performed the best, likely due to its ability to capture non-linear relationships. At k=4, Agglomerative Clustering achieved the highest accuracy, possibly because its hierarchical nature allows it to merge smaller, similar clusters into larger, meaningful groupings that align well with moderate granularity. At k=8, K-Means improved slightly, while Spectral Clustering and Agglomerative Clustering both dropped, probably due to creating overly fragmented groupings that lacked coherence. Overall, Spectral Clustering showed strong performance at lower cluster sizes, Agglomerative Clustering excelled at moderate clusters, and K-Means displayed consistent, albeit lower, performance across all configurations. These results highlight the importance of choosing the right clustering method and cluster size for optimizing accuracy.
            </p>
            <p>
                The runtime comparison of the clustering methods reveals notable differences in computational efficiency. When analyzing time complexity, we ran each clustering algorithm on 5000 embeddings (roughly ¼ of our dataset) and found that K-Means takes 21 seconds, Agglomerative takes 14 seconds, and Spectral takes 119 seconds. Agglomerative clustering was the fastest, with K-Means close behind. Spectral clustering, however, was extremely computationally expensive. Spectral Clustering, while effective at capturing non-linear relationships, is significantly slower than the other methods, which could be a bottleneck for larger datasets or real-time applications. Agglomerative Clustering seems to demonstrate the best balance between accuracy and runtime at k = 4, making it a strong choice for tasks prioritizing speed and hierarchical insights. K-Means remains a good middle-ground option, offering moderate runtime and competitive accuracy, particularly as the number of clusters increases.
            </p>
        </section>

        <section id="conclusion">
            <h2>Conclusion and Future Works</h2>
            <p>
                In this project, we explored multiple clustering techniques—<strong>KMeans</strong>, <strong>Agglomerative Clustering</strong>, and <strong>Spectral Clustering</strong>—to evaluate their performance and efficiency in the context of patch-based emotion recognition. Our results highlighted the trade-offs between clustering quality and computational efficiency. While Agglomerative Clustering demonstrated the best runtime performance, KMeans and Spectral Clustering offered better accuracy under certain conditions.
            </p>
            <p>
                We initially considered using <strong>Gaussian Mixture Models (GMMs)</strong> due to their probabilistic approach to clustering, which assumes embeddings are drawn from a mixture of Gaussian distributions. The Expectation-Maximization (EM) algorithm, a core component of GMMs, provides a powerful mechanism to estimate cluster parameters. However, due to time constraints and the computational complexity of GMMs when applied to large datasets like ours, we chose to omit this method from our final implementation. Future exploration into GMMs, particularly with optimized frameworks or smaller subsets of embeddings, may reveal their potential in capturing subtle patterns in the data.
            </p>
        
            <h3>Future Directions</h3>
            <ul>
                <li><strong>Exploration of GMMs:</strong> Investigating the viability of GMMs using optimized algorithms or smaller embedding subsets to mitigate computational costs.</li>
                <li><strong>Other Clustering Techniques:</strong> Exploring advanced clustering algorithms, such as DBSCAN or hierarchical variations, to better adapt to the dataset's characteristics.</li>
                <li><strong>Parameter Tuning:</strong> Experimenting with key hyperparameters, including the number of transformer heads, depth of the architecture, and attention mechanisms, to optimize clustering performance.</li>
                <li><strong>Larger Number of Clusters:</strong> Investigating how models perform when using a larger number of smaller clusters to capture finer-grained patterns in the data.</li>
                <li><strong>Multi-Modal Extensions:</strong> Combining visual embeddings with other modalities, such as audio or text, to enhance the emotion recognition pipeline.</li>
            </ul>
            <p>
                This work establishes a strong foundation for leveraging clustering methods in vision tasks, with room to integrate advanced clustering techniques and additional modalities in future studies.
            </p>
        </section>

        <section id="references">
        <h2>Acknowledgements and References</h2>
            <p>
                We would like to express our heartfelt gratitude to <strong>Professor Isola</strong>, <strong>Professor Beery</strong>, and <strong>Dr. Bernstein</strong> and the <strong>course staff of 6.7960</strong> for their invaluable guidance, feedback, and support throughout this project. Their insights and expertise have been instrumental in shaping our understanding and execution of this work.
            </p>
            <ul class="references">
                <li>
                    [1] Younis, E.M.G., Mohsen, S., Houssein, E.H. et al. Machine learning for human emotion recognition: a comprehensive review. 
                    <em>Neural Comput & Applic</em> 36, 8901–8947 (2024). 
                    <a href="https://doi.org/10.1007/s00521-024-09426-2" target="_blank">https://doi.org/10.1007/s00521-024-09426-2</a>
                </li>
                <li>
                    [2] M. Dhuheir, A. Albaseer, E. Baccour, A. Erbad, M. Abdallah and M. Hamdi, "Emotion Recognition for Healthcare Surveillance Systems Using Neural Networks: A Survey," 2021 International Wireless Communications and Mobile Computing (IWCMC), Harbin City, China, 2021, pp. 681-687, 
                    <a href="https://doi.org/10.1109/IWCMC51323.2021.9498861" target="_blank">doi: 10.1109/IWCMC51323.2021.9498861</a>.
                </li>
                <li>
                    [3] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. A survey on visual transformer. 
                    <a href="https://arxiv.org/abs/2012.12556" target="_blank">arXiv preprint arXiv:2012.12556</a>, 2020.
                </li>
                <li>
                    [4] Trockman, A., & Kolter, J. Z. Patches are all you need?. 
                    <a href="https://arxiv.org/abs/2201.09792" target="_blank">arXiv preprint arXiv:2201.09792</a>, 2022.
                </li>
                <li>
                    [5] R. Grainger, T. Paniagua, X. Song, N. Cuntoor, M. W. Lee and T. Wu, "PaCa-ViT: Learning Patch-to-Cluster Attention in Vision Transformers," 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada, 2023, pp. 18568-18578, 
                    <a href="https://doi.org/10.1109/CVPR52729.2023.01781" target="_blank">doi: 10.1109/CVPR52729.2023.01781</a>.
                </li>
                <li>
                    [6] Y. Shen, Y. Lu, Y. Luo and J. Ke, "Cluster Image Patches with Multiple Mutual Information in Unlabelled Whole-Slide Image," 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), Houston, TX, USA, 2021, pp. 1509-1512, 
                    <a href="https://doi.org/10.1109/BIBM52615.2021.9669810" target="_blank">doi: 10.1109/BIBM52615.2021.9669810</a>.
                </li>
                <li>
                    [7] Nazeri, S. (2023, July 19). Comparing the-state-of-the-art clustering algorithms. 
                    <a href="https://medium.com/@sina.nazeri/comparing-the-state-of-the-art-clustering-algorithms-1e65a08157a1" target="_blank">Medium</a>.
                </li>
            </ul>
        </section>

    </main>

    <footer>
        <p>&copy; Fall 2024 Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan | Contact: <a href="mailto:ananya_k@mit.edu">ananya_k@mit.edu</a>, <a href="mailto:brishika@mit.edu">brishika@mit.edu</a>, <a href="mailto:sharvaa@mit.edu">sharvaa@mit.edu</a></p>
    </footer>
</body>
</html>

