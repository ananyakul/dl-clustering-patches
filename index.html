<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>6.7960 Final Project Fall 2024</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <header>
        <h1>Patch-Based Clustering Techniques for Facial Emotion Recognition</h1>
        <p>By Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan</p>
    </header>

    <nav>
        <a href="#introduction">Introduction</a>
        <a href="#preprocessing">Data</a>
        <a href="#methods">Methods</a>
        <a href="#experiments">Experiments</a>
        <a href="#conclusion">Conclusion</a>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>This project investigates how different image patch clustering algorithms affect the accuracy of facial emotion recognition. </p>
        </section>

        <section id="preprocessing">
            
            <h2>Dataset and Preprocessing</h2>
            <section>
                <h3 class="subheading">Dataset Description</h3>
                <p>
                    The dataset used for this project is derived from the 
                    <a href="https://www.kaggle.com/datasets/noamsegal/affectnet-training-data" target="_blank">AffectNet Training Data</a>, 
                    one of the largest datasets for facial emotion recognition. It provides labeled examples of human faces for eight primary emotions: 
                    <strong>Anger, Contempt, Disgust, Fear, Happiness, Neutral, Sadness,</strong> and <strong>Surprise</strong>.
                </p>
                <p>
                    AffectNet stands out due to its diversity, with faces captured in real-world environments under varying lighting, poses, and demographics. 
                    This makes it an ideal dataset for training models capable of generalizing across diverse conditions.
                </p>
            </section>
    
            <section>
                <h3 class="subheading">Challenges in the Dataset</h3>
                <p>
                    Despite its richness, AffectNet presents significant challenges:
                </p>
                <ul>
                    <li><strong>Class Imbalance:</strong> Certain emotions like "Happiness" dominate the dataset, while others like "Disgust" are underrepresented.</li>
                    <li><strong>Variability:</strong> Differences in lighting, facial occlusions, and pose angles add noise to the dataset.</li>
                    <li><strong>Subjectivity:</strong> Emotions are inherently subjective, and labeling inconsistencies may arise across annotators.</li>
                </ul>
            </section>
    
            <section>
                <h3 class="subheading">Original Emotion Distribution</h3>
                <p>
                    The original dataset exhibits a skewed distribution across emotion classes, as visualized in the chart below:
                </p>
                <div class="bar-chart">
                    <img src="images/Data_Distribution.png" alt="Bar chart showing original emotion distribution">
                    <p>Figure 1: Original Data Distribution Across Classes</p>
                </div>
            </section>
    
            <section>
                <h3 class="subheading">Balanced Dataset</h3>
                <p>
                    To address the class imbalance, we balanced the dataset by subsampling 
                    <strong>2,477 images</strong> per class, resulting in a total of <strong>19,816 images</strong>. 
                    This ensures fair representation across all emotions and prevents the model from being biased toward overrepresented classes.
                </p>
            </section>
            <section>
                <h3 class="subheading">Example Images</h3>
                <p>
                    Below are sample images from the dataset, showcasing one example from each emotion class, labeled for clarity:
                </p>
                <div class="grid-container grid-4-columns">
                    <div class="grid-item">
                        <img src="images/anger.jpg" alt="Anger Example">
                        <p class="label">Anger</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/contempt.png" alt="Contempt Example">
                        <p class="label">Contempt</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/disgust.jpg" alt="Disgust Example">
                        <p class="label">Disgust</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/fear.jpg" alt="Fear Example">
                        <p class="label">Fear</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/happy.png" alt="Happiness Example">
                        <p class="label">Happiness</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/neutral.png" alt="Neutral Example">
                        <p class="label">Neutral</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/sad.jpg" alt="Sad Example">
                        <p class="label">Sad</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/surprise.png" alt="Surprise Example">
                        <p class="label">Surprise</p>
                    </div>
                </div>
            </section>
            </section>
                <h3 class="subheading">CNN Embeddings</h3>
                <p> 
                    We first ensured all images were indeed of size 96 by 96. After that we split teh image into 36 patches of dimensions 16 by 16 each. Each image was now represented by 36 image patches.
                </p>
                <p>
                    As part of our methodolgy, we utilize a pretrained <strong>ResNet-50</strong> model to generate 
                    2048-dimensional feature embeddings for each image patch. These embeddings are extracted from the 
                    global average pooling layer of the network, which ensures compact and informative representations 
                    of visual features.
                </p>
                <p>
                    These embeddings serve as the foundation for our clustering techniques. By leveraging transfer 
                    learning, we efficiently extract high-level features, reducing computational overhead while 
                    maintaining rich feature representations for the downstream clustering tasks.
                </p>
            </section>

        <section id="methods">
            <h2>Methods</h2>
            <div class="figure-container">
                <div class="figure-item">
                    <img src="images/DL_Arch_Final.png" alt="Model Arch">
                    <p>Figure 2: Architecture Flow</p>
                </div>
            </div>
    <section>
        <h3 class="subheading">Vision Transformer Architecture</h3>
        <p>
            Vision Transformers (ViTs) adapt the traditional transformer model to make them applicable to computer vision tasks. Unlike convolutional neural networks, which rely on local receptive fields, ViTs divide images into fixed-size patches and treat them as a sequence of tokens which are linearly projected into an embedding space. The self-attention mechanism in ViTs then computes relationships between all pairs of patches (patch-to-patch attention). A learnable class token aggregates information from all patches, and its output is used for classification tasks. ViTs excel at capturing long-range dependencies and global patterns in image data, making them particularly effective for tasks requiring detailed contextual understanding. 
        </p>
        <p>
            A well known issue with patch-to-patch attention is that it suffers from quadratic complexity, since every query must interact with every key. Our architecture instead uses Patch-to-Cluster attention, where a predefined number of cluster assignments is first learned and then used in computing the key and value, resulting in linear complexity.
        </p>
        <div class="grid-container grid-2-columns">
            <div class="grid-item">
                <img src="images/Patch-to-Patch.jpg" alt="Patch-to-Patch">
                <p class="label">Patch-to-Patch Attention</p>
            </div>
            <div class="grid-item">
                <img src="images/Patch-to-Cluster.jpg" alt="Patch-to-Cluster">
                <p class="label">Patch-to-Cluster Attention</p>
            </div>
        </div>
    </section>
    <section>
        <h3 class="subheading">Patch-to-Cluster Attention</h3>
        <p>
            Our attention mechanism operates as follows:
        </p>
        <ul>
            <li><strong>Query, Key, and Value Projections:</strong> Patch embeddings are projected into query vectors (Q) using a linear layer. Cluster centroids, computed via a clustering algorithm, are projected into key (K) and value (V) vectors. These projections are used to calculate the relationships between patches and clusters.</li>
            <li><strong>Attention Computation:</strong> Attention scores are computed using a scaled dot-product where N is the number of patches, C is the embedding dimensionality, and M is the number of clusters.
                \[
                    A_{N,M} = \text{Softmax}\left( \frac{Q_{N,C} \cdot K_{M,C}^T}{\sqrt{C}} \right)_{\text{dim}=1}
                \]
            </li>
            <li><strong>Weighted Aggregation:</strong> The attention scores weigh the cluster value vectors (V). This step integrates global cluster-level information into the patch representations.</li>
            <li><strong>Output:</strong> The aggregated patch representations are transformed via a linear output projection to produce the final enhanced patch embeddings.</li>
        </ul>
        <p>Our model architecture diverges from the traditional Vision Transformer design by employing patch-to-cluster attention (See Figure 2).</p>
    </section>
    
    <section>
        <h3 class="subheading">Patch Clustering</h3>
        <ul>
            <li>
                <strong>K-Means Clustering:</strong> iteratively partitions data into a predefined number of clusters by minimizing the variance within each cluster. Centroids are initialized randomly, and clusters are updated based on the distance of each data point to the nearest centroid. K-means is simple and computationally efficient, making it suitable for high-dimensional data. It is most effective for well-separated and spherical clusters.
            </li>
            <li>
                <strong>Agglomerative Clustering:</strong> starts with each data point as its own cluster and recursively merges the most similar clusters until a predefined number is reached. Notably, it is able to capture hierarchical relationships in the data and is also computationally efficient.
            </li>
            <li>
                <strong>Spectral Clustering:</strong> Spectral clustering constructs a similarity graph based on pairwise relationships between data points and performs eigenvector decomposition to embed the graph into a lower-dimensional space for clustering. It is known for handling complex, arbitrarily shaped clusters effectively. It is also suitable for datasets where clusters are not linearly separable.
            </li>
        </ul>
        <div class="grid-container grid-3-columns">
            <div class="grid-item">
                <img src="images/KMeans.png" alt="KMeans Clustering">
            </div>
            <div class="grid-item">
                <img src="images/Agglo.png" alt="Agglomerative Clustering">
            </div>
            <div class="grid-item">
                <img src="images/Spectral.png" alt="Spectral Clustering">
            </div>
        </div>
    </section>

            <section id="model-training">
                <h2>Model Training</h2>
                <ul>
                    <li>
                        <strong>Dataset and Preprocessing:</strong> The dataset is split into training and testing sets, with <strong>80%</strong> of the data used for training and <strong>20%</strong> for testing.
                    </li>
                    <li>
                        <strong>Training Setup:</strong> We train our model with the Adam optimizer using a learning rate of <strong>1e-3</strong>, <strong>CrossEntropyLoss</strong>, a depth of <strong>6</strong>, and <strong>30 epochs</strong> of training.
                    </li>
                    <li>
                        <strong>Device:</strong> Model training is done on <strong>Google Colab A100 GPU</strong>.
                    </li>
                </ul>
            </section>

        
        <section id="experiments">
            <h2>Experiments and Results</h2>
            <section>
                <h3 class="subheading">Experimental Setup</h3>
                <p>
                   We analyze the classification accuracy of our model across three clustering methods (K-means, spectral, and agglomerative) and three cluster sizes (2, 4, and 8). We evaluate the classification accuracies of our models on the AffectNet test set. We also evaluate the time complexity of each clustering method to better understand the tradeoff between accuracy and computational costs.
                </p>
            </section>
            <table class="results-table">
                <thead>
                    <tr>
                        <th>Clusters</th>
                        <th>KMeans</th>
                        <th>Agglomerative</th>
                        <th>Spectral</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>2</td>
                        <td><b>33.65%</b></td>
                        <td>28.71%</td>
                        <td><b>44.00%</b></td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>26.46%</td>
                        <td><b>41.42%</b></td>
                        <td>38.75%</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>31.76%</td>
                        <td>25.38%</td>
                        <td>33.00%</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section id="conclusion">
            <h2>Conclusion and Future Work</h2>
            <p>Our findings highlight the potential for AI-generated music to evolve with more nuanced training. Future work will refine these models and incorporate deeper listener feedback.</p>
        </section>

        <section id="references">
        <h2>Acknowledgements and References</h2>
            <p>
                We would like to express our heartfelt gratitude to <strong>Professor Isola</strong>, <strong>Professor Beery</strong>, and <strong>Dr. Bernstein</strong> and the <strong>course staff of 6.7960</strong> for their invaluable guidance, feedback, and support throughout this project. Their insights and expertise have been instrumental in shaping our understanding and execution of this work.
            </p>
            <ul class="references">
                <li>
                    [1] Younis, E.M.G., Mohsen, S., Houssein, E.H. et al. Machine learning for human emotion recognition: a comprehensive review. 
                    <em>Neural Comput & Applic</em> 36, 8901–8947 (2024). 
                    <a href="https://doi.org/10.1007/s00521-024-09426-2" target="_blank">https://doi.org/10.1007/s00521-024-09426-2</a>
                </li>
            </ul>
        </section>

    </main>

    <footer>
        <p>&copy; Fall 2024 Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan | Contact: <a href="mailto:ananya_k@mit.edu">ananya_k@mit.edu</a>, <a href="mailto:brishika@mit.edu">brishika@mit.edu</a>, <a href="mailto:sharvaa@mit.edu">sharvaa@mit.edu</a></p>
    </footer>
</body>
</html>

