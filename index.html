<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>6.7960 Final Project Fall 2024</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Patch-Based Clustering Techniques for Facial Emotion Recognition</h1>
        <p>By Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan</p>
    </header>

    <nav>
        <a href="#introduction">Introduction</a>
        <a href="#methods">Methods</a>
        <a href="#results">Results</a>
        <a href="#conclusion">Conclusion</a>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>This project investigates how different image patch clustering algorithms affect the accuracy of facial emotion recognition. </p>
        </section>

        <section id="methods">
            
            <h2>Dataset and Preprocessing</h2>
            <section>
                <h3 class="subheading">Dataset Description</h3>
                <p>
                    The dataset used for this project is derived from the 
                    <a href="https://www.kaggle.com/datasets/noamsegal/affectnet-training-data" target="_blank">AffectNet Training Data</a>, 
                    one of the largest datasets for facial emotion recognition. It provides labeled examples of human faces for eight primary emotions: 
                    <strong>Anger, Contempt, Disgust, Fear, Happiness, Neutral, Sadness,</strong> and <strong>Surprise</strong>.
                </p>
                <p>
                    AffectNet stands out due to its diversity, with faces captured in real-world environments under varying lighting, poses, and demographics. 
                    This makes it an ideal dataset for training models capable of generalizing across diverse conditions.
                </p>
            </section>
    
            <section>
                <h3 class="subheading">Challenges in the Dataset</h3>
                <p>
                    Despite its richness, AffectNet presents significant challenges:
                </p>
                <ul>
                    <li><strong>Class Imbalance:</strong> Certain emotions like "Happiness" dominate the dataset, while others like "Disgust" are underrepresented.</li>
                    <li><strong>Variability:</strong> Differences in lighting, facial occlusions, and pose angles add noise to the dataset.</li>
                    <li><strong>Subjectivity:</strong> Emotions are inherently subjective, and labeling inconsistencies may arise across annotators.</li>
                </ul>
            </section>
    
            <section>
                <h3 class="subheading">Original Emotion Distribution</h3>
                <p>
                    The original dataset exhibits a skewed distribution across emotion classes, as visualized in the chart below:
                </p>
                <div class="bar-chart">
                    <img src="images/Data_Distribution.png" alt="Bar chart showing original emotion distribution">
                </div>
            </section>
    
            <section>
                <h3 class="subheading">Balanced Dataset</h3>
                <p>
                    To address the class imbalance, we balanced the dataset by subsampling 
                    <strong>2,477 images</strong> per class, resulting in a total of <strong>19,816 images</strong>. 
                    This ensures fair representation across all emotions and prevents the model from being biased toward overrepresented classes.
                </p>
            </section>
            <section>
                <h3 class="subheading">Example Images</h3>
                <p>
                    Below are sample images from the dataset, showcasing one example from each emotion class, labeled for clarity:
                </p>
                <div class="grid-container">
                    <div class="grid-item">
                        <img src="images/anger.jpg" alt="Anger Example">
                        <p class="label">Anger</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/contempt.png" alt="Contempt Example">
                        <p class="label">Contempt</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/disgust.jpg" alt="Disgust Example">
                        <p class="label">Disgust</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/fear.jpg" alt="Fear Example">
                        <p class="label">Fear</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/happy.png" alt="Happiness Example">
                        <p class="label">Happiness</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/neutral.png" alt="Neutral Example">
                        <p class="label">Neutral</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/sad.jpg" alt="Sad Example">
                        <p class="label">Sad</p>
                    </div>
                    <div class="grid-item">
                        <img src="images/surprise.png" alt="Surprise Example">
                        <p class="label">Surprise</p>
                    </div>
                </div>
            </section>

            <h2>Methods</h2>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 1: Architecture Flow</h3>
                    <img src="images/DL_Arch_Final.png" alt="Model Arch">
                </div>
            </div>
            <p>Our preliminary analysis of the generated embeddings reveals the following insights:</p>
            <ul>
                <li>
                    <b>Clustering Patterns:</b> Both PCA and t-SNE visualizations show distinct regions, 
                    suggesting potential grouping of embeddings based on facial emotion characteristics or metadata 
                    (e.g., specific emotion clusters or labeling differences).
                </li>
                <li>
                    <b>Separation in t-SNE:</b> The t-SNE plot highlights better local relationships, 
                    indicating that embeddings capture nuanced features effectively, making it suitable for identifying distinct emotions.
                </li>
                <li>
                    <b>Outlier Detection:</b> Sparse points in the plots may correspond to outliers or unique embeddings, 
                    potentially representing uncommon emotional patterns or anomalies in the dataset.
                </li>
                <li>
                    <b>Next Steps:</b> Incorporating metadata (e.g., intensity scores or subject identifiers) 
                    as color-coded labels could reveal additional patterns and validate how well embeddings align with these attributes.
                </li>
            </ul>
        </section>

        <section id="results">
            <h1>Clustering Comparison Table</h1>
            <table class="results-table">
                <thead>
                    <tr>
                        <th>Clusters</th>
                        <th>KMeans</th>
                        <th>Agglomerative</th>
                        <th>Spectral</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>2</td>
                        <td>33.65%</td>
                        <td>28.71%</td>
                        <td>44.00%</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>26.46%</td>
                        <td>41.42%</td>
                        <td>38.75%</td>
                    </tr>
                    <tr>
                        <td>8</td>
                        <td>31.76%</td>
                        <td>25.38%</td>
                        <td>33.00%</td>
                    </tr>
                </tbody>
            </table>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 3</h3>
                    <img src="images/placeholder-image.jpg" alt="Media Lab Study Audio">
                </div>
                <div class="figure-item">
                    <h3>Figure 4:</h3>
                    <img src="images/placeholder-image.jpg" alt="Dataset Audio">
                </div>
            </div>
            <p>Our preliminary analysis reveals the following insights:</p>
            <ul>
                <li>AI-generated music tends to have more repetitive patterns.</li>
                <li>Human-composed music demonstrates greater adherence to themes and more emotion.</li>
                <li>Listeners rated AI-generated music as "very generic."</li>
            </ul>
            <p>Our findings show that there is a struggle to distinguish between the pieces and we will further investigate the results as well as potentially run another larger scale study to learn more.</p>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>Our findings highlight the potential for AI-generated music to evolve with more nuanced training. Future work will refine these models and incorporate deeper listener feedback.</p>
        </section>
    </main>

    <footer>
        <p>&copy; Fall 2024 Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan | Contact: <a href="mailto:ananya_k@mit.edu">ananya_k@mit.edu</a>, <a href="mailto:brishika@mit.edu">klecamwa@mit.edu</a>, <a href="mailto:klecamwa@mit.edu">sharvaa@mit.edu</a></p>
    </footer>
</body>
</html>

