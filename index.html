<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Supplementary Page</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Patch-Based Clustering Techniques for Facial Emotion Recognition</h1>
        <p>By Ananya Kulshrestha, Rishika Bansal, Sharvaa Selvan</p>
    </header>

    <nav>
        <a href="#introduction">Introduction</a>
        <a href="#methods">Methods</a>
        <a href="#results">Results</a>
        <a href="#conclusion">Conclusion</a>
    </nav>

    <main>
        <section id="introduction">
            <h2>Introduction</h2>
            <p>This project investigates the ability to predict whether a piece of music is AI-generated or human-composed Our analysis aims to identify key features that differentiate these compositions and explore how AI can impact the sound of music.</p>
        </section>

        <section id="methods">
            <h2>Methods</h2>
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 1: PCA Visualization of Embeddings</h3>
                    <img src="images/embeddings_figures/PCA_embeddings.png" alt="PCA Visualization">
                </div>
                <div class="figure-item">
                    <h3>Figure 2: t-SNE Visualization of Embeddings</h3>
                    <img src="images/embeddings_figures/t-SNE_embeddings.png" alt="t-SNE Visualization">
                </div>
            </div>
            <p>Our preliminary analysis of the generated embeddings reveals the following insights:</p>
            <ul>
                <li><b>Clustering Patterns:</b> Both PCA and t-SNE visualizations show distinct regions, suggesting potential grouping of embeddings based on musical characteristics or metadata (e.g., genre or human vs. AI-generated labels).
</li>
                <li><b>Separation in t-SNE:</b> The t-SNE plot highlights better local relationships, indicating that embeddings capture nuanced features effectively, making it suitable for identifying similar audio samples.
</li>
                <li><b>Outlier Detection:</b> Sparse points in the plots may correspond to outliers or unique embeddings, potentially representing uncommon musical patterns or anomalies in the dataset.
</li>
                <li><b>Next Steps:</b> Incorporating metadata (e.g., genres, repetition scores) as color-coded labels could reveal additional patterns and validate how well embeddings align with these attributes.
</li>
            </ul>
        </section>

        <section id="results">
            <div class="figure-container">
                <div class="figure-item">
                    <h3>Figure 3: Media Lab Study Audio</h3>
                    <img src="images/class_study/ML_Audio_Pilot.png" alt="Media Lab Study Audio">
                </div>
                <div class="figure-item">
                    <h3>Figure 4: Dataset Audio</h3>
                    <img src="images/class_study/Dataset_Audio_Pilot.png" alt="Dataset Audio">
                </div>
            </div>
            <p>Our preliminary analysis of the class survey reveals the following insights:</p>
            <ul>
                <li>AI-generated music tends to have more repetitive patterns.</li>
                <li>Human-composed music demonstrates greater adherence to themes and more emotion.</li>
                <li>Listeners rated AI-generated music as "very generic."</li>
            </ul>
            <p>Our findings show that there is a struggle to distinguish between the pieces and we will further investigate the results as well as potentially run another larger scale study to learn more.</p>
        </section>

        <section id="conclusion">
            <h2>Conclusion</h2>
            <p>Our findings highlight the potential for AI-generated music to evolve with more nuanced training. Future work will refine these models and incorporate deeper listener feedback.</p>
        </section>
    </main>

    <footer>
        <p>&copy; Fall 2024 Ananya Kulshrestha and Kimaya Lecamwasam | Contact: <a href="mailto:ananya_k@mit.edu">ananya_k@mit.edu</a> and <a href="mailto:klecamwa@mit.edu">klecamwa@mit.edu</a></p>
    </footer>
</body>
</html>

